def tokenise(input: str) -> list[str]:
    def regular_tokeniser(input: str) -> list[str]:
        return input.replace("(", " ( ").replace(")", " ) ").strip().split()
    
    def single_quote_tokeniser(input: str) -> list[str]:
        # Your existing logic for handling one pair
        quote_indices = [i for i, c in enumerate(input) if c == '"']
        substring_preserved = input[quote_indices[0] : quote_indices[1] + 1]
        substring1 = input[: quote_indices[0]]
        substring2 = input[quote_indices[1] + 1 :]
        return (
            regular_tokeniser(substring1)
            + [substring_preserved]
            + tokenise(substring2)  # Recursive composition
        )
    
    if not input:
        return []
    if '"' not in input:
        return regular_tokeniser(input)
    
    quote_count = input.count('"')
    if quote_count % 2 != 0:
        # Unpaired quotes - handle as regular tokens
        return regular_tokeniser(input)
    
    return single_quote_tokeniser(input)
